
********************************************************************************
********* Practical Session 1: Datamining For Big Data *************************
********************************************************************************

You can do this exercises in groups of at most 2 peoples.  You need to
complete this file with your answers and upload it in the "M2 CS Big data
(BIG DATA)" course on Claroline connect.

****** Fill in the student names in your group in the next two lines: (do not modify or remove this line)
- Abdul Rahman Zakarya
- Muaz Twaty
****** end names (do not modify or remove this line)


The aim of this session is to practice with the spark framework.
You will use the interactive pyspark shell.
It is available in the spark installation directory (in the "bin" sub-directory).

****** TODO
To start this practical session, you should:
- create a folder "big_data" in your home directory.
- download the data.zip archive on claroline connect and unzip it in the big_data directory.
- copy this file (session1.txt) in this "big_data" directory and open it with a text editor.
- launch a terminal in this directory.
- launch the pyspark shell in the terminal (type the following in the terminal):

/home/jeudbapt/local/spark-2.3.1-bin-hadoop2.7/bin/pyspark --master local[4]

In case of error, it may be necessary to type:
export SPARK_LOCAL_IP=127.0.0.1
in the terminal before launching pyspark.

The "--master local[4]" option means that spark is started on the local machine on 4 cores.
By default, the pyspark shell uses python version 2.7, if you want to use version 3 of python,
you have to type
export PYSPARK_PYTHON=python3
in the terminal before launching pyspark.


Except for the questions of part I, you should copy in this file the code that you type in the interactive shell. 


********************************************************************************
***************  Some documentation:
********************************************************************************

 You will use Spark version 2.3 or 2.4.

 This version provides another data structure (DataFrames) in addition to
 RDDs, but you will use RDD (RDD are less powerful but simpler).

 The Quick Start guide for Spark version 2.3 is mainly focused on
 DataFrames and not RDDs.
 Therefore, you will use the Quick start guide of version 2.0.

 The Spark start page (version 2.0):
 http://spark.apache.org/docs/2.0.0/quick-start.html

 Python tutorial:
 https://docs.python.org/3/tutorial/index.html

 Spark documentation on RDDs:
   http://spark.apache.org/docs/latest/rdd-programming-guide.html
  the available Spark operations on RDDs:
  - transformations:
    http://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations
  - actions:
    http://spark.apache.org/docs/latest/rdd-programming-guide.html#actions

  - The full documentation of the pyspark package (with examples): 
    http://spark.apache.org/docs/latest/api/python/pyspark.html 


********************************************************************************
*****************  Part I
********************************************************************************

1) Go on the quick start page:
http://spark.apache.org/docs/2.0.0/quick-start.html
Do the "Basic" part on the "lotr.txt" text file (this text file is in the archive on claroline).

2) Do the "more on RDD" operations using again the "lotr.txt" text file.

3) Compute the list of (word, count) sorted by decreasing number of occurrences. What are the 5 most frequent words and their number of occurrences ?
('the', 406), ('and', 383), ('of', 235), ('a', 207), ('to', 205)

********************************************************************************
************************* IMPORTANT ******************************************** 
********************************************************************************

For the next questions, you have to copy in this file the code that you
type in the interactive shell between the lines "******* answer x.x" and
"******* end answer x.x".  Do not modify or remove these two lines (they
are used to automatically extract your answers).
********************************************************************************


********************************************************************************
******************  Part II
********************************************************************************

For these questions, you will need the following operations on RDD:
- map
- flatmap
- reduceByKey
- filter
- take
- collect


The file products.txt  contains a list of products.
Each line of this file is:
TID product_name price category

1) Generate a RDD of tuples (tid, name, price, category) from this text file.
   The "tid" and "price" should be integers. You can use the function int() to convert a string to an integer (e.g., int("12") is 12).
******* answer 2.1: do not modify or remove this line
txtProducts = sc.textFile("products.txt")
def mapper(line):
    splt = line.split()
    return (int(splt[0]), splt[1], int(splt[2]), splt[3])

txtProducts.map(mapper).collect()
******* end answer 2.1: do not modify or remove this line

2) Generate a RDD with all products with price > 20.
******* answer 2.2
txtProducts.map(mapper).filter(lambda x: x[2]>20).collect()
******* end answer 2.2


3) Generate a RDD with the maximum price for each category of product.
******* answer 2.3
def category_price_mapper(line):
    splt = line.split()
    return (splt[3], int(splt[2]))

txtProducts.map(category_price_mapper).reduceByKey(max).collect()
******* end answer 2.3


4) The number of products in each category
******* answer 2.4
def category_count_mapper(line):
    splt = line.split()
    return (splt[3], 1)

txtProducts.map(category_count_mapper).reduceByKey(lambda a, b: a+b).collect()
******* end answer 2.4


5) The average price of products in each category.
******* answer 2.5
def maper(x):
    xx = x.split()
    return ( xx[3], (1,int(xx[2])) )

def reducer(a,b):
    return ( a[0]+b[0] , (a[0]*a[1] + b[0]*b[1]) / (a[0]+b[0]) )

txtProducts.map(maper).reduceByKey(reducer).map(lambda x: (x[0],x[1][1])).collect()

# Another Solution:

def category_price_count_mapper(line):
    splt = line.split()
    return (splt[3], [int(splt[2]), 1])

txtProducts.map(category_price_count_mapper)\
    .reduceByKey(lambda a, b: [sum(x) for x in zip(a, b)])\
    .map(lambda t: (t[0], t[1][0]/t[1][1])).collect()
******* end answer 2.5


********************************************************************************
*************** Part III
********************************************************************************
In this part, you may also need operations union(), distinct() and intersect().


 In this part you will use the files A.txt and B.txt that are
 sets/multisets of integers (one integer per line).  The aim of this part
 is to compute operations (union, intersection,...) on sets or multi-sets
 using Spark.

 In questions 1 and 2, we consider that we are working on sets.
 This means that each element should appear at most once in the result.

1) Compute the union of A and B (First, you need to build RDDs for sets A
   and B)
******* answer 3.1
A = sc.textFile("A.txt").map(lambda x: int(x)).distinct()
B = sc.textFile("B.txt").map(lambda x: int(x)).distinct()
A.union(B).distinct().collect()
******* end answer 3.1

2) Compute the intersection of A and B:
******* answer 3.2
A = sc.textFile("A.txt").map(lambda x: int(x)).distinct()
B = sc.textFile("B.txt").map(lambda x: int(x)).distinct()
A.intersection(B).collect()
******* end answer 3.2


 We can define all these operations on multisets also. A multiset is a set
 where an element can appears several times.

3) Union (if an element appear a times in A and b times in B, it must
   appear a+b times in the union)
******* answer 3.3
A = sc.textFile("A.txt").map(lambda x: int(x))
B = sc.textFile("B.txt").map(lambda x: int(x))
A.union(B).collect()
******* end answer 3.3

4) Intersection (if an element appear a times in A and b times in B, it
   must appear min(a,b) times in the intersection)
******* answer 3.4
A = sc.textFile("A.txt").map(lambda x: (int(x), (1, +1))).reduceByKey(lambda a, b: (a[0] + b[0], +1))
B = sc.textFile("B.txt").map(lambda x: (int(x), (1, -1))).reduceByKey(lambda a, b: (a[0] + b[0], -1))
I = A.union(B).reduceByKey(lambda a, b: (min(a[0],b[0]), a[1]+b[1])).filter(lambda x: x[1][1] == 0).map(lambda x: (x[0], x[1][0]))
I.flatMap(lambda x: [x[0]] * x[1]).collect()
******* end answer 3.4

For the following, it may be interesting to buid a RDD that contains
elements of the form (el, (a,b)) where "el" is an element of the multi-set,
"a" its number of occurrences in A and "b" its number of occurrences in B.

5) Difference (if an element appear a times in A and b times in B, it must
   appear max(0,a-b) times in the difference)
******* answer 3.5
A = sc.textFile("A.txt").map(lambda x: (int(x), 1))
B = sc.textFile("B.txt").map(lambda x: (int(x), -1))
A = A.reduceByKey(lambda a, b: a+b)
B = B.reduceByKey(lambda a, b: a+b)
D = A.union(B).reduceByKey(lambda a, b: a+b).filter(lambda x: x[1] > 0)
D.flatMap(lambda x: [x[0]] * x[1]).collect()
******* end answer 3.5

6) Symmetric difference (if an element appear a times in A and b times in
   B, it must appear max(a,b) - min(a,b) in the symmetric difference)
******* answer 3.6
A = sc.textFile("A.txt").map(lambda x: (int(x), 1))
B = sc.textFile("B.txt").map(lambda x: (int(x), 1))
A = A.reduceByKey(lambda a, b: a+b)
B = B.reduceByKey(lambda a, b: a+b)
SD = A.union(B).reduceByKey(lambda a, b: max(a,b) - min(a,b))
SD.flatMap(lambda x: [x[0]] * x[1]).collect()
******* end answer 3.6


